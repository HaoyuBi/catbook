---
title: Hospital Readmission Project Report
comments: true
---

# EXECUTIVE SUMMARY
The Affordable Care Act (ACA) established the Hospital Readmission Reduction Program (HRRP) in 2012. Under this program, hospitals are financially penalized if they have higher than expected risk-standardized 30-day readmission rates for acute myocardial infarction, heart failure, and pneumonia. More than half of the nation’s hospitals were affected by CMS<sup>1</sup> penalties in FY 2017. Cost of Penalties Across US Hospitals Increased from $290 million in 2013 to $528 million in 2017. A 2016 study on hospital profitability published in the journal Health Affairs found that most the median acute care hospital is losing $82 per discharge. In addition, hospitals also face operations management issues if they cannot predict returning patients accurately. The operational inefficiency leads to patient dissatisfaction and may also lead to increasing in patient return rate.

Tackling the dire need of hospital to reduce their readmission rate, the report looks to utilize data analysis to provide solution that enables hospitals to implement necessary interventions or plan decisively for returning patients. The report is split into two parts, the first part lists necessary interventions and profiles of returning patients (so that they can be broadly identified) based on exploratory, inferential and clustering methods. The second part allows hospital to predict returning patients and plan their operations efficiently.

Based on the insights derived from logistic regression and exploratory analysis, we suggest that the hospital should try to identify reasons behind higher chances of return rate among patients arriving on Sunday. This may require the hospital to adopt a customized continuum of care plan for patients arriving on Sunday. Similarly, patients arriving in the early part of the day seems to be have higher chances of return. The reason could be that more chronic patients arrive during day time. If true, the hospital can consider adding more specialized staff in the morning shift. We also found that patients with government sponsored programs such as Medicare and Medicaid seem to have higher return rate. However, their chances of return decreases with the money charged to them. We believe that maybe paying extra money is making the government sponsored patients to avoid unnecessary hospital visits. The hospital can look to incentivize government sponsored patients to reduce their visits as their visits seem to be impacted by monetary benefits/loss. In addition, we found that return rate is high for patients who leave prior to completing treatment, without signing discharge instruction, LWBS before triage, LWBS after triage or elopement. Some of the potential reasons for this could be overcrowding, inefficient flow of patients or staff shortage. We recommend the hospital considering capacity expansion, revisiting SOP (standard operating procedures), or following up regularly with such patients so that they remain on the path to recovery.

To identify profile of returning patients, we used both K-means Clustering clustering method. We identified that returning patients belong to 4 main profiles. We believe that the hospital should build customized care plan designed for each of these patient profiles.

Finally, we have built a state-of-the-art staked model based on 4 ensemble models as base learners and one ensemble as level 2 learner. At a testing accuracy of 79.03%, the stacked model had the highest accuracy of all models, further detailed in the sections below. We envision that the model will allow the hospitals to predict returning patients and plan operationally for them. In addition, we suggest that the hospital must also update the model when new data is available.

-------

# EXPLORATORY AND INFERENTIAL ANALYSIS
In this section, we looked at not only exploring data and identifying potential data imputation methods, but also tackled two key questions related to the main business problem: 1. What intervention hospital can implement to reduce return rate and; 2. Identify profile of people who are likely to return, which will help in development of customized care plan for such patients.

We are limiting data cleaning in this section to training data as we would like to study relationship between return and other variables. The data cleaning on complete dataset is discussed in the next section. We started by replacing ‘ ’, ‘#N/A’, ‘#VALUE’ entries to NA for consistency. For the variables (testing and training data combined) CONSULT_IN_ED (97.6% missing), RISK (86.5% missing), SEVERITY (86.5% missing), and ADMIT_RESULT (80.1% missing), we have categorized the missing values as a new category as we believe that the missing values are probably not recorded or measured. Next, wherever appropriate, we converted variables to factors. For example, we converted variable SAME_DAY (0 or 1) to factor variable. We also deleted MONTH_DEP and HOUR_DEP variables as they had perfect collinearity with MONTH_ARR and HOUR_ARR variables, respectively. Thereafter, we deleted rows with missing values and were left with 90.30% of the observation, enough to not be concerned about imputation of missing values.

After data cleaning, we tried multiple logistic regression models as they are appropriate for drawing inferences (association of response with other variables). After rigorous trial and error, we found a logistic regression with additional interaction terms (gender\*age; financial_class\*charges; and severity*charges) had the lowest AIC and hence, it formed the main basis of our inferential summary. Following are some  findings and suggestions for the hospital:

*	Male patients with older age have a higher return rate on average compared to the female patients. We think that old male patients can be an important group to pay attention to and propose to make more frequent follow ups on them to lower their return rate.
*	Increase in charge seems to decrease the chances of return rate for patients whose severity is not measured. We believe that the hospital should measure this variable in the future to ascertain the interaction term conclusively as we have replaced missing values with “not measured”  category.
*	Increase in charge seems to reduce chances of return for patients belonging to government sponsored programs such as Medicaid & Medicare groups. We believe that  paying extra money is making the government sponsored patients to avoid later hospital visits.
*	Patients arriving on Sunday seems to have higher chances of return compared to other weekdays. The hospital must make necessary intervention after identifying the reason behind high patient return rate among patients arriving on Sunday. Similar trend is observed for patients visiting in the  early part of a day.
*	Patients who are leaving hospital without following a normal routine i.e. those leaving without completing treatment, elopement, or without signing discharge instructions seem to have high return rate. We believe that the reasons behind patient leaving hospital without following a normal routine may leads to identification of areas of improvement for the hospital. These could include capacity expansion, revisiting SOP (standard operating procedures), or regular follow-ups.

![Image text](https://github.com/HaoyuBi/haoyubi.github.io/assets/img/Picture1.png）

Finally, we used K-means Clustering to build a broad profile of returning patients. From the scree plot below, we believe that we should potentially consider four main returning patients’ profiles as it results in sufficient explanation of variance in the data. Lastly, following are broad profiles of returning patients based on the center of clusters:

*	Male black African American who does not consult specialty physician and whose risk is not generally measured; he belongs to MCO medicaid or military financial class and incurs low charges during hospital visits; his severity of distress is categorized at less urgent
*	Old white patient who belongs to commercial or medicare financial class with emergent condition and generally a trauma inpatient; average charges incurred for this group is high
*	Native Hawaiian who has generally a global contract and measured at high risk and severity
*	Male with medicaid-pending or state-medicaid financial class; severity of distress is measured at urgent and kept in observation after leaving emergency room


 
# MODELING AND MODEL EVALUATION
One of the business objectives is to precisely predict returning patients. While, true positive rate is expected to be a good candidate metric to evaluate prediction, we have focused on accuracy, as TPR doesn’t include misclassification in negative samples and we also don’t want to classify too many non-returning patients as returning patients in pursuit of improving TPR.

Next step was to focus on imputing missing values. We started by replacing ‘  ’, ‘#N/A’, ‘#VALUE’ entries to NA for consistency. For the variables (test and train data combined) CONSULT_IN_ED (97.6% missing), RISK (86.3% missing), SEVERITY (86.3% missing), and ADMIT_RESULT (79.9% missing), we have categorized the missing values as a new category because we believe that the missing values are probably not recorded or measured. Next, wherever appropriate, we converted variables to factors. For example, we converted variable SAME_DAY (0 or 1) to factor variable. Figure below provides details of missing values by variable before imputation.


In addition, we did feature engineering on DC_RESULT, which had many categories with only a limited number of values. We have binned categories with limited number of observations. We expected that limited observations for a category may impact the working of algorithms especially the QDA, which requires a certain number of observations in each category as it calculates mean and variance by categories in a variable. Refer to appendix for details of number of observations by category of the DC_RESULT variable.

After completing the above-mentioned steps, we impute the missing values in the remaining categories using predictive mean matching method using mice package in R. Predictive mean matching builds linear regression using the variables to be imputed as response variable and all other variables as predictor variables. The method uses posterior predictive distribution (distribution of possible unobserved values conditional on the observed values) to identify multiple β’s of the regression/logistic equation and suggests multiple potential candidate values for missing entries.

Once, we had cleaned and completed data (with no missing values, appropriate variable type, and appropriately binned), we implemented predictive models such as classification tree, KNN, logistic regression, LDA, QDA. However, accuracy of all these methods on holdout/testing dataset was below 77.5%, slightly above the baseline.

As a result, we decided to try ensemble methods in pursuit of improving prediction accuracy. Before we implement ensemble methods, we used model.matix() function in R to convert categorical variables to dummy variables. This was not necessary for all methods, but we wanted to use same data structures for all models to compare holdout accuracy. We started with random forest and bagging ensembles based on trees, which build multiple trees on bootstrapped sample of the data and final prediction is the combination of all prediction by each individual tree. Random forest uncorrelated trees by allowing random selection of features at tree split. We used ranger package in R to implement random forest. Ranger is a fast implementation of random forest. The first step was to optimize the parameters as they can have significant impact on prediction accuracy. We focused on optimizing four parameters - sample.fraction (fraction of observations to sample with replacement, this can prevent overfitting ), mtry (number of variables randomly selected at each split decision, helps in building uncorrelated trees), num.trees(number of trees , large number of trees can overfit especially for noisy datasets), min_node_size (minimum number of observation in a node, this parameter implicitly sets the depth of trees and can be useful to avoid overfitting). We used cartesian search on a grid of 1782 rows containing various combination of parameters. We ran random forest algorithm 1782 times to identify best parameters  (based on out of bag (OOB) misclassification rate) and found that the lowest OOB misclassification rate was for num.tress = 5000; mtry= 35; min_node_size=35; sample.faction=0.8. The figure below plots OOB misclassification rate vs various combinations of sample.faction and mtry used.

We used the best parameters (obtained above) to build trees on complete training dataset except on the holding data, which we kept separate to compare accuracy of different ensemble models we are going to build. Nonetheless, we found that the holdout accuracy of the prediction increased drastically from 77.5% to 78.5%.

In order to improve accuracy further, we focused on implementing gradient boosting method (GBM) based on trees, which is useful to correctly classify weakly learned observations by increasing their weights. Boosting is also known as a method to convert weak learners into strong learners. It begins with training a classification tree algorithm on original dataset and gives greater weightage to misclassified data points in the subsequent steps to build trees. Final prediction is combination of predictions by all trees. However, this additional step adds more parameters to the model, which also require tuning/optimization. The new parameters are mainly related to the learning rate. We used GBM implementation in h2o package as it allows parallel computation and hence computation is faster.  We used cartesian search on a grid of 70 rows containing various combinations of parameters. We ran GBM algorithm 70 times to identify best parameters  (based on lowest misclassification rate) and found that the lowest misclassification rate was for col_sample_rate=1; learn_rate=0.1; max_depth=7; min_rows=1; ntrees=1000; sample_rate=0.5. We used the best parameters (obtained above) to build trees on complete training dataset except holding data. We found that the holdout accuracy of the prediction increased drastically from 78.5% to 78.81%.

Thereafter, we implemented extreme gradient boosting algorithm (Xgboost) based on trees, considered one of the best algorithms for prediction accuracy and used by many to win data challenge competition at Kaggle. Xgboost is regularized GBM. As a result, regularization parameters (gamma, lambda, alpha) are added to the list of parameters for tuning. We used xgb.cv function in the xgboost package in R to optimize parameters based on cross validation. We used cartesian search on a grid of ~50,000 rows containing various combinations of parameters. We found that the lowest misclassification rate was for eta=0.15, subsample=1, max_depth=500, alpha=0.95, lambda=0.08, gamma=3, min_child_weight=10, max_delta_step=5, colsample_bytree=0.8. We used the best parameters (obtained above) to build trees on complete training dataset except holding data. We found that the holdout accuracy of the prediction increased further from 78.7 to 78.96%.

Finally, we implemented a model-stacking method that used predictions from base learners (xgboost, random forest, GBM, logistic regression) to be used as variable of a new model (xgboost), which is used for final prediction. We split dataset into three parts and used the first part to train base learners and used the model to make prediction for second and third part of the dataset. Then, we trained new model on the second part and used it to make prediction on the third part, which is the holdout set. We found that the holdout accuracy of the prediction increased further from 78.9 to 79.03%.

Our best model, the stacked model, was trained on complete training dataset and available for prediction on an unseen dataset. We suggest that the hospital should regularly update the model when  new data are gathered.
